{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPUs\n",
    "GPUを使う上での，CPUとの使い分けやメモリ使用量等のコントロール方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## どのデバイスを使って計算しているかログで確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22.  28.]\n",
      " [ 49.  64.]]\n"
     ]
    }
   ],
   "source": [
    "# Creates a graph.\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なお，jupyter-notebookだと表示されない模様．\n",
    "通常であれば以下のような出力が得られる．\n",
    "```\n",
    "Device mapping:\n",
    "/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: Tesla K40c, pci bus\n",
    "id: 0000:05:00.0\n",
    "b: /job:localhost/replica:0/task:0/gpu:0\n",
    "a: /job:localhost/replica:0/task:0/gpu:0\n",
    "MatMul: /job:localhost/replica:0/task:0/gpu:0\n",
    "[[ 22.  28.]\n",
    " [ 49.  64.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用するデバイスを手動で設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22.  28.]\n",
      " [ 49.  64.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常であれば以下のような出力が得られる．\n",
    "```\n",
    "Device mapping:\n",
    "/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: Tesla K40c, pci bus\n",
    "id: 0000:05:00.0\n",
    "b: /job:localhost/replica:0/task:0/cpu:0\n",
    "a: /job:localhost/replica:0/task:0/cpu:0\n",
    "MatMul: /job:localhost/replica:0/task:0/gpu:0\n",
    "[[ 22.  28.]\n",
    " [ 49.  64.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU使用メモリの制御\n",
    "デフォルトではTensorFlowはほぼ全てのGPUをmapする．\n",
    "しかし，使用可能なGPUリソースの一部だけを使用したい場合や，\n",
    "必要最低限だけ使用したい場合がある．\n",
    "TensorFlowにはこうした要求に対応可能な制御オプションがある．\n",
    "\n",
    "1つは`allow_growth`オプションを使用する方法．\n",
    "この方法は必要に応じてメモリを確保していく．\n",
    "\n",
    "```sh\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config, ...)\n",
    "```\n",
    "\n",
    "2つめは`per_process_gpu_memory_fraction`オプションを使用する方法．\n",
    "この方法は利用可能なGPUの各合計メモリのうち，何%を使うか決められる．\n",
    "例えば，各GPUの合計メモリの40%を使用するように制御できる．\n",
    "```sh\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## マルチGPUシステムのうちシングルでGPUを使う\n",
    "使用するGPUをすればOK\n",
    "\n",
    "```python\n",
    "# Creates a graph.\n",
    "with tf.device('/gpu:2'):\n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "  c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))\n",
    "```\n",
    "ただし，指定したデバイスがなければ`InvalidArgumentError`となる．\n",
    "\n",
    "```sh\n",
    "InvalidArgumentError: Invalid argument: Cannot assign a device to node 'b':\n",
    "Could not satisfy explicit device specification '/gpu:2'\n",
    "   [[Node: b = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [3,2]\n",
    "   values: 1 2 3...>, _device=\"/gpu:2\"]()]]\n",
    "```\n",
    "\n",
    "上記のような問題を回避するために，デバイスがなかった場合にTensorFlowに自動的にデバイス選択させる場合には`allow_soft_placement`を`True`にする．\n",
    "\n",
    "```python\n",
    "# Creates a graph.\n",
    "with tf.device('/gpu:2'):\n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "  c = tf.matmul(a, b)\n",
    "# Creates a session with allow_soft_placement and log_device_placement set\n",
    "# to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(\n",
    "      allow_soft_placement=True, log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## マルチGPUを使う\n",
    "\n",
    "使用するGPUを複数指定すればよい?\n",
    "\n",
    "```python\n",
    "# Creates a graph.\n",
    "c = []\n",
    "for d in ['/gpu:2', '/gpu:3']:\n",
    "  with tf.device(d):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])\n",
    "    c.append(tf.matmul(a, b))\n",
    "with tf.device('/cpu:0'):\n",
    "  sum = tf.add_n(c)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(sum))\n",
    "```\n",
    "\n",
    "以下のような結果が得られる\n",
    "\n",
    "```\n",
    "Device mapping:\n",
    "/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: Tesla K20m, pci bus\n",
    "id: 0000:02:00.0\n",
    "/job:localhost/replica:0/task:0/gpu:1 -> device: 1, name: Tesla K20m, pci bus\n",
    "id: 0000:03:00.0\n",
    "/job:localhost/replica:0/task:0/gpu:2 -> device: 2, name: Tesla K20m, pci bus\n",
    "id: 0000:83:00.0\n",
    "/job:localhost/replica:0/task:0/gpu:3 -> device: 3, name: Tesla K20m, pci bus\n",
    "id: 0000:84:00.0\n",
    "Const_3: /job:localhost/replica:0/task:0/gpu:3\n",
    "Const_2: /job:localhost/replica:0/task:0/gpu:3\n",
    "MatMul_1: /job:localhost/replica:0/task:0/gpu:3\n",
    "Const_1: /job:localhost/replica:0/task:0/gpu:2\n",
    "Const: /job:localhost/replica:0/task:0/gpu:2\n",
    "MatMul: /job:localhost/replica:0/task:0/gpu:2\n",
    "AddN: /job:localhost/replica:0/task:0/cpu:0\n",
    "[[  44.   56.]\n",
    " [  98.  128.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
